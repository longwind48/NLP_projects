{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST446 Distributed Computing for Big Data\n",
    "## Homework 1: Spark RDDs, Spark SQL and Hive\n",
    "### Milan Vojnovic and Christine Yuen, LT 2018\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "**Deadline**: February 20, 2018, 5pm London time\n",
    "\n",
    "**Datasets**: All the datasets are available for download from here:\n",
    "\n",
    "https://www.dropbox.com/sh/89xbpcjl4oq0j4w/AACrbtUzm3oCW1OcpL7BasRfa?dl=0.\n",
    "\n",
    "\n",
    "## A. Spark RDDs (30 points)\n",
    "\n",
    "1. We continue to analyse the dblp dataset available in the file `large_author.txt`. This time we want to find the top 10 author pairs who jointly published the largest number of papers (with possible other collaborators). For example, if authors _a_, _b_ and _c_ published a paper with title _t_, then this contributes one joint publication for each author pair (_a_,_b_), (_b_,_c_) and (_a_,_c_). Use the first column of the input data for the author names and use the third column of the input data for the publication title. You need to solve this task by using RDD operations like those in _rdd.ipynb_ in week 3 of the course and the [Spark RDD documentation]( http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD). You need to run your code on your laptop.\n",
    "\n",
    "2. Run the same code as in the previous item but on Google Cloud Platform. Provide us with a copy and paste of the terminal commands that you used as well as with screenshots if you run this using a web user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Sudhakar M. Reddy', 'Irith Pomeranz'), 247),\n",
       " (('Divyakant Agrawal', 'Amr El Abbadi'), 161),\n",
       " (('Makoto Takizawa', 'Tomoya Enokido'), 138),\n",
       " (('Henri Prade', 'Didier Dubois'), 122),\n",
       " (('Tharam S. Dillon', 'Elizabeth Chang'), 116),\n",
       " (('Mary Jane Irwin', 'Narayanan Vijaykrishnan'), 107),\n",
       " (('Mahmut T. Kandemir', 'Mary Jane Irwin'), 100),\n",
       " (('Chun Chen', 'Jiajun Bu'), 99),\n",
       " (('Takahiro Hara', 'Shojiro Nishio'), 96),\n",
       " (('Maurizio Lenzerini', 'Giuseppe De Giacomo'), 91)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('1A') \\\n",
    ".set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    ".set(\"spark.kryoserializer.buffer.max\", \"128m\") \\\n",
    ".set(\"spark.kryoserializer.buffer\", \"64m\") \n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "textrdd = sc.textFile(\"file///C:/hduser/author-large.txt\")\n",
    "\n",
    "rdd1 = textrdd.map(lambda x: x.split(\"\\t\", 1)) \\\n",
    "            .map(lambda x: (x[1],x[0]))\n",
    "\n",
    "# Make a copy of the rdd    \n",
    "rdd1_copy = rdd1\n",
    "\n",
    "# Do a join on both rdds to make pairs\n",
    "rddop = rdd1.join(rdd1_copy)\n",
    "\n",
    "# Outputs a list which shows top 20 pairs according to their counts, in descending order.\n",
    "rddop_1 = rddop.map(lambda x: x[1]) \\\n",
    "               .filter(lambda x: x[0]!=x[1]) \\\n",
    "               .map(lambda x: (x,1)) \\\n",
    "               .reduceByKey(lambda x,y: x+y) \\\n",
    "               .takeOrdered(20, key = lambda x: -x[1])\n",
    "\n",
    "# Define a funciton to remove mirror duplicates\n",
    "def removeMirorDups(list):\n",
    "        emptylist = []\n",
    "        for p1 in range(0, len(list)):\n",
    "                for p2 in range(p1+1,len(list)):\n",
    "                    if (list[p1][0][0]==list[p2][0][1] and list[p1][0][1]==list[p2][0][0]):\n",
    "                        emptylist.append(list[p1])\n",
    "        return emptylist\n",
    "\n",
    "# Apply function to the list, assign the output to a new list variable\n",
    "rddop_2 = removeMirorDups(rddop_1)\n",
    "\n",
    "# Outputs top 10 author pairs\n",
    "rddop_2[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Run the same code as in the previous item but on Google Cloud Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I uploaded author-large.txt into my bucket.\n",
    "- Then SSH into my Cloud Dataproc cluster's master node to access the terminal window that is connected to the master instance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](gcp_pyspark0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code line by line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](gcp_pyspark1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Spark SQL (30 points)\n",
    "\n",
    "Do the same as in problem A except that now you need to use Spark SQL API, which we covered in week 4 of the course. You may find useful to consult 'Querying with Spark SQL' in _spark-dataframe-sql.ipynb_ of week 4 class and the [Spark SQL documentation](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+-----+\n",
      "|author_name        |author_name2           |count|\n",
      "+-------------------+-----------------------+-----+\n",
      "|Irith Pomeranz     |Sudhakar M. Reddy      |247  |\n",
      "|Amr El Abbadi      |Divyakant Agrawal      |161  |\n",
      "|Tomoya Enokido     |Makoto Takizawa        |138  |\n",
      "|Henri Prade        |Didier Dubois          |122  |\n",
      "|Tharam S. Dillon   |Elizabeth Chang        |116  |\n",
      "|Mary Jane Irwin    |Narayanan Vijaykrishnan|107  |\n",
      "|Mahmut T. Kandemir |Mary Jane Irwin        |100  |\n",
      "|Chun Chen          |Jiajun Bu              |99   |\n",
      "|Takahiro Hara      |Shojiro Nishio         |96   |\n",
      "|Giuseppe De Giacomo|Maurizio Lenzerini     |91   |\n",
      "+-------------------+-----------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"Homework_1b\") \\\n",
    "       .getOrCreate()\n",
    "\n",
    "\n",
    "textrdd = sc.textFile(\"file///C:/hduser/author-large.txt\") \\\n",
    "            .map(lambda x: x.strip().split(\"\\t\",1)) \\\n",
    "            .map(lambda x: Row(author_name=x[0],book_name=x[1]))\n",
    "\n",
    "# Convert rdd to dataframe\n",
    "textdf = sqlContext.createDataFrame(textrdd)\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "textdf_copy = textdf\n",
    "\n",
    "# Change column name from 'author_name' to 'author_name2'\n",
    "textdf_copy = textdf_copy.withColumnRenamed('author_name', 'author_name2')\n",
    "\n",
    "# Do a full outer join on the dataframes to make author name pairs,\n",
    "# remove all rows which have pairs with the same names.\n",
    "textdf_pairs = textdf.join(textdf_copy, 'book_name', 'outer')\n",
    "textdf_pairs = textdf_pairs.filter(textdf_pairs['author_name']!=textdf_pairs['author_name2'])\n",
    "\n",
    "# Do a count on the author pairs, in descending order.\n",
    "textdf_pairs_2 = textdf_pairs.groupby('author_name','author_name2').count() \\\n",
    "                             .orderBy('count',ascending=False)\n",
    "\n",
    "# Setting window parameter\n",
    "my_window = Window.partitionBy().orderBy(textdf_pairs_2['count'].desc())\n",
    "\n",
    "# Create a new column named 'prev_value', it stores previous row's author_name2' entry in the current row, \n",
    "# using the row configurations defined by window parameter\n",
    "textdf_pairs_3 = textdf_pairs_2.withColumn(\"prev_row_name\", F.lag(textdf_pairs_2.author_name2).over(my_window))\n",
    "\n",
    "# Create a new column named 'remove', it outputs 1 if entry of author_name is the same as prev_row_name, 0 otherwise.\n",
    "# In other words, remove=1 means there exists a mirror duplicate of author name pairs.\n",
    "textdf_pairs_4 = textdf_pairs_3.withColumn(\"remove\", F.when(textdf_pairs_3.author_name == textdf_pairs_3.prev_row_name, 1) \\\n",
    "                                                          .otherwise(0))\n",
    "\n",
    "# Remove all rows with 'remove'=1, and show only 3 columns.\n",
    "textdf_pairs_5 = textdf_pairs_4.filter(textdf_pairs_4['remove']==0) \\\n",
    "                                .select('author_name', 'author_name2', 'count')\n",
    "\n",
    "textdf_pairs_5.show(10, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Hive (40 points)\n",
    "\n",
    "In this part we are going to use the Yelp data available in the following JSON file *Yelp/yelp_academic_dataset_user.json*. You may complete this task by using either Hive installed on your laptop or using Hive on Google Cloud Platform. You need to complete the following steps:\n",
    "\n",
    "### 1. Load data into a Hive table\n",
    "\n",
    "Create a Hive table and load the input data into this table.\n",
    "\n",
    "Please describe any commmands that you run in a command line interface and provide all the code that you wrote and run. For example, this may include any commands run in a terminal, Hive script files (\\*.sql), and screenshots (if, for example, you used Google Cloud Platform). See the class examples for references.\n",
    "\n",
    "Note:\n",
    "* The dataset is in JSON format whereas in the class the datasets were in XML or TXT format. You need to figure out how to load data from a JSON file to a Hive table. \n",
    "* You need to infer the schema by looking at the data. \n",
    "\n",
    "Hints: \n",
    "\n",
    "* Some of the columns are of array type. For example, you should use array&lt;STRING&gt; for friends column.\n",
    "* The size of the dataset is large (about 1GB). You may want to create a smaller dataset first and work this smaller dataset until you develop and test your code, and then apply it on the original dataset.\n",
    "\n",
    "\n",
    "### 2. Simple queries\n",
    "\n",
    "Having created the Hive table and loaded the data into it, write and execute queries to:\n",
    "\n",
    "i. retrieve the schema;\n",
    "\n",
    "ii. show the number of rows in the table;\n",
    "\n",
    "iii. select top 10 users who have provided the largest number of reviews (the output should consist of the user name and the number of reviews of the users).\n",
    "\n",
    "For all the queries, please show both the commands you used and the output. You may copy and paste the commands that you run and the outputs, or provide screenshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\longwind48\\AppData\\Local\\Google\\Cloud SDK>gcloud config set project project-longwind48\n",
    "Updated property [core/project]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\longwind48\\AppData\\Local\\Google\\Cloud SDK>gcloud config list\n",
    "[compute]\n",
    "region = europe-west1\n",
    "zone = europe-west1-d\n",
    "[core]\n",
    "account = longwind48@gmail.com\n",
    "disable_usage_reporting = True\n",
    "project = project-longwind48\n",
    "\n",
    "Your active configuration is: [default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\longwind48\\AppData\\Local\\Google\\Cloud SDK>gsutil mb gs://tracilim-bucket/\n",
    "Creating gs://tracilim-bucket/..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the dataset into my bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\longwind48\\AppData\\Local\\Google\\Cloud SDK>gsutil cp C:\\hduser\\yelp_academic_dataset_user.json gs://tracilim-bucket/data/yelp_academic_dataset_user.json\n",
    "Copying file://C:\\hduser\\yelp_academic_dataset_user.json [Content-Type=application/octet-stream]...\n",
    "==> NOTE: You are uploading one or more large file(s), which would run\n",
    "significantly faster if you enable parallel composite uploads. This\n",
    "feature can be enabled by editing the\n",
    "\"parallel_composite_upload_threshold\" value in your .boto\n",
    "configuration file. However, note that if you do this large files will\n",
    "be uploaded as `composite objects\n",
    "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
    "means that any user who downloads such objects will need to have a\n",
    "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
    "without a compiled crcmod, computing checksums on composite objects is\n",
    "so slow that gsutil disables downloads of composite objects.\n",
    "\n",
    "\\ [1 files][  1.1 GiB/  1.1 GiB]    9.6 MiB/s\n",
    "Operation completed over 1 objects/1.1 GiB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a cluster named 'mycluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\longwind48\\AppData\\Local\\Google\\Cloud SDK>gcloud dataproc clusters create mycluster --project project-longwind48 --bucket tracilim-bucket\n",
    "API [dataproc.googleapis.com] not enabled on project\n",
    "[project-longwind48]. Would you like to enable and retry?  (y/N)?  y\n",
    "\n",
    "Enabling service dataproc.googleapis.com on project project-longwind48...\n",
    "Waiting for async operation operations/tmo-acf.2cb981fd-cbc2-419e-aae5-70026f4c7f3c to complete...\n",
    "Operation finished successfully. The following command can describe the Operation details:\n",
    " gcloud services operations describe operations/tmo-acf.2cb981fd-cbc2-419e-aae5-70026f4c7f3c\n",
    "Waiting on operation [projects/project-longwind48/regions/global/operations/9b1e37cc-5500-47d6-b815-9d431ef9cba0].\n",
    "Waiting for cluster creation operation...done.\n",
    "Created [https://dataproc.googleapis.com/v1/projects/project-longwind48/regions/global/clusters/mycluster] Cluster placed in zone [europe-west1-d]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\longwind48\\AppData\\Local\\Google\\Cloud SDK>gcloud dataproc jobs submit hive --cluster mycluster -e \"create table json_yelp1 (json string);\"\n",
    "Job [de2fffc3-71d3-42e2-a3c1-368b37c1d043] submitted.\n",
    "Waiting for job output...\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/usr/lib/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
    "Connecting to jdbc:hive2://mycluster-m:10000\n",
    "Connected to: Apache Hive (version 2.1.1)\n",
    "Driver: Hive JDBC (version 2.1.1)\n",
    "18/02/14 17:40:49 [main]: WARN jdbc.HiveConnection: Request to set autoCommit to false; Hive does not support autoCommit=false.\n",
    "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
    "No rows affected (0.144 seconds)\n",
    "Beeline version 2.1.1 by Apache Hive\n",
    "Closing: 0: jdbc:hive2://mycluster-m:10000\n",
    "Job [de2fffc3-71d3-42e2-a3c1-368b37c1d043] finished successfully.\n",
    "driverControlFilesUri: gs://tracilim-bucket/google-cloud-dataproc-metainfo/d8d61f95-861b-42e2-9f8f-b8222e02f5a0/jobs/de2fffc3-71d3-42e2-a3c1-368b37c1d043/\n",
    "driverOutputResourceUri: gs://tracilim-bucket/google-cloud-dataproc-metainfo/d8d61f95-861b-42e2-9f8f-b8222e02f5a0/jobs/de2fffc3-71d3-42e2-a3c1-368b37c1d043/driveroutput\n",
    "hiveJob:\n",
    "  queryList:\n",
    "    queries:\n",
    "    - create table json_yelp1 (json string);\n",
    "placement:\n",
    "  clusterName: mycluster\n",
    "  clusterUuid: d8d61f95-861b-42e2-9f8f-b8222e02f5a0\n",
    "reference:\n",
    "  jobId: de2fffc3-71d3-42e2-a3c1-368b37c1d043\n",
    "  projectId: project-longwind48\n",
    "status:\n",
    "  state: DONE\n",
    "  stateStartTime: '2018-02-14T17:40:51.047Z'\n",
    "statusHistory:\n",
    "- state: PENDING\n",
    "  stateStartTime: '2018-02-14T17:40:43.220Z'\n",
    "- state: SETUP_DONE\n",
    "  stateStartTime: '2018-02-14T17:40:44.095Z'\n",
    "- details: Agent reported job success\n",
    "  state: RUNNING\n",
    "  stateStartTime: '2018-02-14T17:40:45.164Z'\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\longwind48\\AppData\\Local\\Google\\Cloud SDK>gcloud dataproc jobs submit hive --cluster mycluster -e \"load data inpath 'gs://tracilim-bucket/data/yelp_academic_dataset_user.json' INTO TABLE json_yelp1;\"\n",
    "Job [89038e2d-14b0-476d-94a3-51344f891aea] submitted.\n",
    "Waiting for job output...\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/usr/lib/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
    "Connecting to jdbc:hive2://mycluster-m:10000\n",
    "Connected to: Apache Hive (version 2.1.1)\n",
    "Driver: Hive JDBC (version 2.1.1)\n",
    "18/02/14 17:42:37 [main]: WARN jdbc.HiveConnection: Request to set autoCommit to false; Hive does not support autoCommit=false.\n",
    "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
    "Error: Error while compiling statement: FAILED: SemanticException [Error 10028]: Line 1:23 Path is not legal ''gs://tracilim-bucket/data/yelp_academic_dataset_user.json'': Source file system should be \"file\" if \"local\" is specified (state=42000,code=10028)\n",
    "Closing: 0: jdbc:hive2://mycluster-m:10000\n",
    "ERROR: (gcloud.dataproc.jobs.submit.hive) Job [89038e2d-14b0-476d-94a3-51344f891aea] entered state [ERROR] while waiting for [DONE]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\longwind48\\AppData\\Local\\Google\\Cloud SDK>gcloud dataproc jobs submit hive --cluster mycluster -e \"describe json_yelp1;\"\n",
    "Job [7677d21c-0f6a-469e-89e0-d3efdd5a6e59] submitted.\n",
    "Waiting for job output...\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/usr/lib/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
    "Connecting to jdbc:hive2://mycluster-m:10000\n",
    "Connected to: Apache Hive (version 2.1.1)\n",
    "Driver: Hive JDBC (version 2.1.1)\n",
    "18/02/14 17:43:54 [main]: WARN jdbc.HiveConnection: Request to set autoCommit to false; Hive does not support autoCommit=false.\n",
    "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
    "+-----------+------------+----------+--+\n",
    "| col_name  | data_type  | comment  |\n",
    "+-----------+------------+----------+--+\n",
    "| json      | string     |          |\n",
    "+-----------+------------+----------+--+\n",
    "1 row selected (0.132 seconds)\n",
    "Beeline version 2.1.1 by Apache Hive\n",
    "Closing: 0: jdbc:hive2://mycluster-m:10000\n",
    "Job [7677d21c-0f6a-469e-89e0-d3efdd5a6e59] finished successfully.\n",
    "driverControlFilesUri: gs://tracilim-bucket/google-cloud-dataproc-metainfo/d8d61f95-861b-42e2-9f8f-b8222e02f5a0/jobs/7677d21c-0f6a-469e-89e0-d3efdd5a6e59/\n",
    "driverOutputResourceUri: gs://tracilim-bucket/google-cloud-dataproc-metainfo/d8d61f95-861b-42e2-9f8f-b8222e02f5a0/jobs/7677d21c-0f6a-469e-89e0-d3efdd5a6e59/driveroutput\n",
    "hiveJob:\n",
    "  queryList:\n",
    "    queries:\n",
    "    - describe json_yelp1;\n",
    "placement:\n",
    "  clusterName: mycluster\n",
    "  clusterUuid: d8d61f95-861b-42e2-9f8f-b8222e02f5a0\n",
    "reference:\n",
    "  jobId: 7677d21c-0f6a-469e-89e0-d3efdd5a6e59\n",
    "  projectId: project-longwind48\n",
    "status:\n",
    "  state: DONE\n",
    "  stateStartTime: '2018-02-14T17:43:55.924Z'\n",
    "statusHistory:\n",
    "- state: PENDING\n",
    "  stateStartTime: '2018-02-14T17:43:48.710Z'\n",
    "- state: SETUP_DONE\n",
    "  stateStartTime: '2018-02-14T17:43:49.231Z'\n",
    "- details: Agent reported job success\n",
    "  state: RUNNING\n",
    "  stateStartTime: '2018-02-14T17:43:49.948Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the number of rows in the table: 1029432 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\longwind48\\AppData\\Local\\Google\\Cloud SDK>gcloud dataproc jobs submit hive --cluster mycluster -e \"select count(*) from json_yelp1;\"\n",
    "Job [b07ef972-af76-49a1-aca4-012eb0b38909] submitted.\n",
    "Waiting for job output...\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/usr/lib/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
    "Connecting to jdbc:hive2://mycluster-m:10000\n",
    "Connected to: Apache Hive (version 2.1.1)\n",
    "Driver: Hive JDBC (version 2.1.1)\n",
    "18/02/14 17:46:33 [main]: WARN jdbc.HiveConnection: Request to set autoCommit to false; Hive does not support autoCommit=false.\n",
    "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
    "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
    "+----------+--+\n",
    "|    c0    |\n",
    "+----------+--+\n",
    "| 1029432  |\n",
    "+----------+--+\n",
    "1 row selected (31.176 seconds)\n",
    "Beeline version 2.1.1 by Apache Hive\n",
    "Closing: 0: jdbc:hive2://mycluster-m:10000\n",
    "Job [b07ef972-af76-49a1-aca4-012eb0b38909] finished successfully.\n",
    "driverControlFilesUri: gs://tracilim-bucket/google-cloud-dataproc-metainfo/d8d61f95-861b-42e2-9f8f-b8222e02f5a0/jobs/b07ef972-af76-49a1-aca4-012eb0b38909/\n",
    "driverOutputResourceUri: gs://tracilim-bucket/google-cloud-dataproc-metainfo/d8d61f95-861b-42e2-9f8f-b8222e02f5a0/jobs/b07ef972-af76-49a1-aca4-012eb0b38909/driveroutput\n",
    "hiveJob:\n",
    "  queryList:\n",
    "    queries:\n",
    "    - select count(*) from json_yelp1;\n",
    "placement:\n",
    "  clusterName: mycluster\n",
    "  clusterUuid: d8d61f95-861b-42e2-9f8f-b8222e02f5a0\n",
    "reference:\n",
    "  jobId: b07ef972-af76-49a1-aca4-012eb0b38909\n",
    "  projectId: project-longwind48\n",
    "status:\n",
    "  state: DONE\n",
    "  stateStartTime: '2018-02-14T17:47:08.717Z'\n",
    "statusHistory:\n",
    "- state: PENDING\n",
    "  stateStartTime: '2018-02-14T17:46:27.135Z'\n",
    "- state: SETUP_DONE\n",
    "  stateStartTime: '2018-02-14T17:46:27.652Z'\n",
    "- details: Agent reported job success\n",
    "  state: RUNNING\n",
    "  stateStartTime: '2018-02-14T17:46:28.746Z'\n",
    "yarnApplications:\n",
    "- name: select count(*) from json_yelp1(Stage-1)\n",
    "  progress: 1.0\n",
    "  state: FINISHED\n",
    "  trackingUrl: http://mycluster-m:8088/proxy/application_1518622563983_0001/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select top 10 users who have provided the largest number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\longwind48\\AppData\\Local\\Google\\Cloud SDK>gcloud dataproc jobs submit hive --cluster mycluster -e \"select get_json_object(json_yelp1.json, '$.name') as name, get_json_object(json_yelp1.json, '$.review_count') as review_count from json_yelp1 ORDER BY review_count DESC LIMIT 10;\"\n",
    "Job [239dd811-e573-4014-ba16-ab308b6ccc48] submitted.\n",
    "Waiting for job output...\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/usr/lib/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
    "Connecting to jdbc:hive2://mycluster-m:10000\n",
    "Connected to: Apache Hive (version 2.1.1)\n",
    "Driver: Hive JDBC (version 2.1.1)\n",
    "18/02/14 17:59:38 [main]: WARN jdbc.HiveConnection: Request to set autoCommit to false; Hive does not support autoCommit=false.\n",
    "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
    "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
    "+-----------+---------------+--+\n",
    "|   name    | review_count  |\n",
    "+-----------+---------------+--+\n",
    "| Michelle  | 999           |\n",
    "| claudia   | 997           |\n",
    "| Julie     | 997           |\n",
    "| Kathy     | 996           |\n",
    "| Dan       | 996           |\n",
    "| Nobbi     | 996           |\n",
    "| Allison   | 996           |\n",
    "| Meghan    | 996           |\n",
    "| Tiffany   | 994           |\n",
    "| Tiffany   | 994           |\n",
    "+-----------+---------------+--+\n",
    "10 rows selected (32.771 seconds)\n",
    "Beeline version 2.1.1 by Apache Hive\n",
    "Closing: 0: jdbc:hive2://mycluster-m:10000\n",
    "Job [239dd811-e573-4014-ba16-ab308b6ccc48] finished successfully.\n",
    "driverControlFilesUri: gs://tracilim-bucket/google-cloud-dataproc-metainfo/d8d61f95-861b-42e2-9f8f-b8222e02f5a0/jobs/239dd811-e573-4014-ba16-ab308b6ccc48/\n",
    "driverOutputResourceUri: gs://tracilim-bucket/google-cloud-dataproc-metainfo/d8d61f95-861b-42e2-9f8f-b8222e02f5a0/jobs/239dd811-e573-4014-ba16-ab308b6ccc48/driveroutput\n",
    "hiveJob:\n",
    "  queryList:\n",
    "    queries:\n",
    "    - select get_json_object(json_yelp1.json, '$.name') as name, get_json_object(json_yelp1.json,\n",
    "      '$.review_count') as review_count from json_yelp1 ORDER BY review_count DESC\n",
    "      LIMIT 10;\n",
    "placement:\n",
    "  clusterName: mycluster\n",
    "  clusterUuid: d8d61f95-861b-42e2-9f8f-b8222e02f5a0\n",
    "reference:\n",
    "  jobId: 239dd811-e573-4014-ba16-ab308b6ccc48\n",
    "  projectId: project-longwind48\n",
    "status:\n",
    "  state: DONE\n",
    "  stateStartTime: '2018-02-14T18:00:14.090Z'\n",
    "statusHistory:\n",
    "- state: PENDING\n",
    "  stateStartTime: '2018-02-14T17:59:32.313Z'\n",
    "- state: SETUP_DONE\n",
    "  stateStartTime: '2018-02-14T17:59:33.144Z'\n",
    "- details: Agent reported job success\n",
    "  state: RUNNING\n",
    "  stateStartTime: '2018-02-14T17:59:33.881Z'\n",
    "yarnApplications:\n",
    "- name: select get_json_object(json_yelp1.json,...10(Stage-1)\n",
    "  progress: 1.0\n",
    "  state: FINISHED\n",
    "  trackingUrl: http://mycluster-m:8088/proxy/application_1518622563983_0003/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
